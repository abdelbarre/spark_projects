{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and Write into FTP using Spark (Pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init (Don't modify this code, it is mandatory for both Development and Production modes)\n",
    "APPLICATION_NAME = \"ftp_test\"\n",
    "import os, sys\n",
    "def delPyc(pycFile):\n",
    "    os.remove(pycFile + \".pyc\") if os.path.exists(pycFile + \".pyc\") else None\n",
    "try: APPLICATION_HOME = os.environ[\"APPLICATION_HOME\"]\n",
    "except: APPLICATION_HOME = \"..\"\n",
    "delPyc(APPLICATION_HOME + \"/lib/SparkInit\")\n",
    "sys.path.append(os.path.abspath(APPLICATION_HOME + \"/lib\"))\n",
    "from SparkInit import *  ## check the lib dir\n",
    "from SparkFTP import *  ## check the lib dir\n",
    "\n",
    "## initiate the SparkContext\n",
    "try: spark\n",
    "except: spark = None\n",
    "try: sc\n",
    "except: sc = None\n",
    "sparkInit = SparkInit(spark, sc, APPLICATION_HOME, DEFAULT_APPLICATION_NAME=APPLICATION_NAME)\n",
    "spark = sparkInit.spark\n",
    "sc = sparkInit.sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading From FTP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sparkFtp = SparkFTP(sc)\n",
    "## FTP Connection \n",
    "sparkFtp.openFtpConnection(\"your_host\", 21, \"user\", \"password\") ## Try to use your credentials \n",
    "## Creating DF using FTP csv (example of file my_example.csv)\n",
    "ftpfilesI =  sparkFtp.scanFtpFiles(\"/\", recursive=False, startsWith=None, contains=None, endsWith=\".csv\") # \"/\" is the path dir\n",
    "for line in ftpfilesI:\n",
    "    if line == \"/my_example.csv\":\n",
    "            ftpDataframePath = line\n",
    "            df_ftp = sparkFtp.ftpCsvFileToSparkDf(ftpDataframePath, isHeader=True, columns=None, columnDelimiter=',', lineDelimiter='\\n')\n",
    "            if df_ftp is None :\n",
    "                raise Exception(\"File 'my_example.csv' is empty\"+ftpDataframePath+\".\")   \n",
    "            break \n",
    "    else : \n",
    "        raise Exception('No file found with name my_example.csv check if the file existe in network space')    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------+\n",
      "|Index|Height(Inches)|Weight(Pounds)|\n",
      "+-----+--------------+--------------+\n",
      "|    1|      65.78331|      112.9925|\n",
      "|    2|      71.51521|      136.4873|\n",
      "|    3|      69.39874|      153.0269|\n",
      "|    4|       68.2166|      142.3354|\n",
      "|    5|      67.78781|      144.2971|\n",
      "|    6|      68.69784|      123.3024|\n",
      "|    7|      69.80204|      141.4947|\n",
      "|    8|      70.01472|      136.4623|\n",
      "|    9|      67.90265|      112.3723|\n",
      "|   10|      66.78236|      120.6672|\n",
      "|   11|      66.48769|      127.4516|\n",
      "|   12|      67.62333|       114.143|\n",
      "|   13|      68.30248|      125.6107|\n",
      "|   14|      67.11656|      122.4618|\n",
      "|   15|      68.27967|      116.0866|\n",
      "|   16|       71.0916|      139.9975|\n",
      "|   17|        66.461|      129.5023|\n",
      "|   18|      68.64927|      142.9733|\n",
      "|   19|      71.23033|      137.9025|\n",
      "|   20|      67.13118|      124.0449|\n",
      "+-----+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ftp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing into FTP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We will add a new column into our Df and we xill name it extra\n",
    "## And then we will write this new Df in the file \"my_example_enriched.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "df_ftp_new = df_ftp.withColumn(\"extra\",F.lit(\"100\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------+-----+\n",
      "|Index|Height(Inches)|Weight(Pounds)|extra|\n",
      "+-----+--------------+--------------+-----+\n",
      "|    1|      65.78331|      112.9925|  100|\n",
      "|    2|      71.51521|      136.4873|  100|\n",
      "|    3|      69.39874|      153.0269|  100|\n",
      "|    4|       68.2166|      142.3354|  100|\n",
      "|    5|      67.78781|      144.2971|  100|\n",
      "|    6|      68.69784|      123.3024|  100|\n",
      "|    7|      69.80204|      141.4947|  100|\n",
      "|    8|      70.01472|      136.4623|  100|\n",
      "|    9|      67.90265|      112.3723|  100|\n",
      "|   10|      66.78236|      120.6672|  100|\n",
      "|   11|      66.48769|      127.4516|  100|\n",
      "|   12|      67.62333|       114.143|  100|\n",
      "|   13|      68.30248|      125.6107|  100|\n",
      "|   14|      67.11656|      122.4618|  100|\n",
      "|   15|      68.27967|      116.0866|  100|\n",
      "|   16|       71.0916|      139.9975|  100|\n",
      "|   17|        66.461|      129.5023|  100|\n",
      "|   18|      68.64927|      142.9733|  100|\n",
      "|   19|      71.23033|      137.9025|  100|\n",
      "|   20|      67.13118|      124.0449|  100|\n",
      "+-----+--------------+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## The new DF\n",
    "df_ftp_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write code\n",
    "sparkFtp.sparkDfToFtpCsvFile(df_ftp_new, \"/my_example_enriched.csv\", isHeader=True, columnDelimiter=',',lineDelimiter='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List the present files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating DF using FTP csv (example of file my_example.csv)\n",
    "ftp_files =  sparkFtp.scanFtpFiles(\"/\", recursive=False, startsWith=None, contains=None, endsWith=\".csv\") # \"/\" is the path dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/my_example.csv', '/my_example_enriched.csv']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ftp_files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-2 Spark-2.4.4 Yarn (Datalab)",
   "language": "python",
   "name": "datalab-python-2-spark-2.4.4-yarn-datalab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
